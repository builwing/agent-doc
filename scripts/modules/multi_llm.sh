#!/usr/bin/env bash
# Ë§áÊï∞LLM„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÂØæÂøú„Ç∑„Çπ„ÉÜ„É†
set -euo pipefail

echo "ü§ñ Ë§áÊï∞LLM„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÂØæÂøú„Çí„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó‰∏≠..."

# 1. „Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†
mkdir -p llm/{providers,strategies,config}

# 2. Áµ±‰∏ÄLLM„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ
cat > llm/llm-manager.js << 'LLMMANAGER_EOF'
#!/usr/bin/env node
/**
 * Multi-LLM Provider Manager
 * Ë§áÊï∞„ÅÆLLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÇíÁµ±‰∏Ä„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÅßÁÆ°ÁêÜ
 */

import { Anthropic } from '@anthropic-ai/sdk';
import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { HfInference } from '@huggingface/inference';
import Replicate from 'replicate';
import chalk from 'chalk';
import ora from 'ora';
import { promises as fs } from 'fs';

// „Éó„É≠„Éê„Ç§„ÉÄ„ÉºÂü∫Â∫ï„ÇØ„É©„Çπ
class LLMProvider {
    constructor(name, config = {}) {
        this.name = name;
        this.config = config;
        this.metrics = {
            totalRequests: 0,
            totalTokens: 0,
            totalLatency: 0,
            errors: 0
        };
    }

    async complete(prompt, options = {}) {
        throw new Error('Subclass must implement complete()');
    }

    async embed(text) {
        throw new Error('Embedding not supported by this provider');
    }

    getMetrics() {
        return {
            ...this.metrics,
            avgLatency: this.metrics.totalRequests > 0 
                ? this.metrics.totalLatency / this.metrics.totalRequests 
                : 0
        };
    }

    recordMetrics(tokens, latency, error = false) {
        this.metrics.totalRequests++;
        this.metrics.totalTokens += tokens;
        this.metrics.totalLatency += latency;
        if (error) this.metrics.errors++;
    }
}

// Anthropic Claude „Éó„É≠„Éê„Ç§„ÉÄ„Éº
class ClaudeProvider extends LLMProvider {
    constructor(config) {
        super('Claude', config);
        this.client = new Anthropic({
            apiKey: config.apiKey || process.env.ANTHROPIC_API_KEY
        });
        this.models = {
            fast: 'claude-3-haiku-20240307',
            balanced: 'claude-3-sonnet-20240229',
            powerful: 'claude-3-opus-20240229'
        };
    }

    async complete(prompt, options = {}) {
        const startTime = Date.now();
        const model = this.models[options.quality || 'balanced'];

        try {
            const response = await this.client.messages.create({
                model,
                max_tokens: options.maxTokens || 1000,
                temperature: options.temperature || 0.7,
                messages: [{ role: 'user', content: prompt }],
                ...options.extra
            });

            const latency = Date.now() - startTime;
            const content = response.content[0].text;
            const tokens = response.usage?.total_tokens || 0;

            this.recordMetrics(tokens, latency);

            return {
                content,
                model,
                provider: this.name,
                usage: {
                    promptTokens: response.usage?.input_tokens || 0,
                    completionTokens: response.usage?.output_tokens || 0,
                    totalTokens: tokens
                },
                latency
            };
        } catch (error) {
            this.recordMetrics(0, Date.now() - startTime, true);
            throw error;
        }
    }
}

// OpenAI GPT „Éó„É≠„Éê„Ç§„ÉÄ„Éº
class OpenAIProvider extends LLMProvider {
    constructor(config) {
        super('OpenAI', config);
        this.client = new OpenAI({
            apiKey: config.apiKey || process.env.OPENAI_API_KEY
        });
        this.models = {
            fast: 'gpt-3.5-turbo',
            balanced: 'gpt-4-turbo-preview',
            powerful: 'gpt-4'
        };
    }

    async complete(prompt, options = {}) {
        const startTime = Date.now();
        const model = this.models[options.quality || 'balanced'];

        try {
            const response = await this.client.chat.completions.create({
                model,
                messages: [{ role: 'user', content: prompt }],
                max_tokens: options.maxTokens || 1000,
                temperature: options.temperature || 0.7,
                ...options.extra
            });

            const latency = Date.now() - startTime;
            const content = response.choices[0].message.content;
            const usage = response.usage;

            this.recordMetrics(usage?.total_tokens || 0, latency);

            return {
                content,
                model,
                provider: this.name,
                usage: {
                    promptTokens: usage?.prompt_tokens || 0,
                    completionTokens: usage?.completion_tokens || 0,
                    totalTokens: usage?.total_tokens || 0
                },
                latency
            };
        } catch (error) {
            this.recordMetrics(0, Date.now() - startTime, true);
            throw error;
        }
    }

    async embed(text) {
        const response = await this.client.embeddings.create({
            model: 'text-embedding-ada-002',
            input: text
        });

        return response.data[0].embedding;
    }
}

// Google Gemini „Éó„É≠„Éê„Ç§„ÉÄ„Éº
class GeminiProvider extends LLMProvider {
    constructor(config) {
        super('Gemini', config);
        this.client = new GoogleGenerativeAI(
            config.apiKey || process.env.GOOGLE_API_KEY
        );
        this.models = {
            fast: 'gemini-1.5-flash',
            balanced: 'gemini-1.5-pro',
            powerful: 'gemini-1.5-pro'
        };
    }

    async complete(prompt, options = {}) {
        const startTime = Date.now();
        const modelName = this.models[options.quality || 'balanced'];
        const model = this.client.getGenerativeModel({ model: modelName });

        try {
            const result = await model.generateContent(prompt);
            const response = await result.response;
            const content = response.text();
            const latency = Date.now() - startTime;

            this.recordMetrics(0, latency); // Gemini„ÅØ‰ΩøÁî®Èáè„ÇíËøî„Åï„Å™„ÅÑ

            return {
                content,
                model: modelName,
                provider: this.name,
                usage: {
                    promptTokens: 0,
                    completionTokens: 0,
                    totalTokens: 0
                },
                latency
            };
        } catch (error) {
            this.recordMetrics(0, Date.now() - startTime, true);
            throw error;
        }
    }
}

// HuggingFace „Éó„É≠„Éê„Ç§„ÉÄ„ÉºÔºà„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„É¢„Éá„É´Ôºâ
class HuggingFaceProvider extends LLMProvider {
    constructor(config) {
        super('HuggingFace', config);
        this.client = new HfInference(
            config.apiKey || process.env.HUGGINGFACE_API_KEY
        );
        this.models = {
            fast: 'microsoft/phi-2',
            balanced: 'mistralai/Mixtral-8x7B-Instruct-v0.1',
            powerful: 'meta-llama/Llama-2-70b-chat-hf'
        };
    }

    async complete(prompt, options = {}) {
        const startTime = Date.now();
        const model = this.models[options.quality || 'balanced'];

        try {
            const response = await this.client.textGeneration({
                model,
                inputs: prompt,
                parameters: {
                    max_new_tokens: options.maxTokens || 1000,
                    temperature: options.temperature || 0.7,
                    ...options.extra
                }
            });

            const latency = Date.now() - startTime;
            const content = response.generated_text;

            this.recordMetrics(0, latency);

            return {
                content,
                model,
                provider: this.name,
                usage: {
                    promptTokens: 0,
                    completionTokens: 0,
                    totalTokens: 0
                },
                latency
            };
        } catch (error) {
            this.recordMetrics(0, Date.now() - startTime, true);
            throw error;
        }
    }
}

// „É≠„Éº„Ç´„É´LLM„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÔºàOllamaÔºâ
class OllamaProvider extends LLMProvider {
    constructor(config) {
        super('Ollama', config);
        this.baseUrl = config.baseUrl || 'http://localhost:11434';
        this.models = {
            fast: 'phi',
            balanced: 'mistral',
            powerful: 'llama2:70b'
        };
    }

    async complete(prompt, options = {}) {
        const startTime = Date.now();
        const model = this.models[options.quality || 'balanced'];

        try {
            const response = await fetch(`${this.baseUrl}/api/generate`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    model,
                    prompt,
                    stream: false,
                    options: {
                        temperature: options.temperature || 0.7,
                        num_predict: options.maxTokens || 1000
                    }
                })
            });

            const data = await response.json();
            const latency = Date.now() - startTime;

            this.recordMetrics(0, latency);

            return {
                content: data.response,
                model,
                provider: this.name,
                usage: {
                    promptTokens: 0,
                    completionTokens: 0,
                    totalTokens: 0
                },
                latency
            };
        } catch (error) {
            this.recordMetrics(0, Date.now() - startTime, true);
            throw error;
        }
    }
}

// LLM„Éû„Éç„Éº„Ç∏„É£„Éº
class LLMManager {
    constructor() {
        this.providers = new Map();
        this.strategies = {
            failover: new FailoverStrategy(),
            loadBalance: new LoadBalanceStrategy(),
            costOptimize: new CostOptimizeStrategy(),
            quality: new QualityFirstStrategy(),
            speed: new SpeedFirstStrategy()
        };
        this.currentStrategy = 'failover';
        this.history = [];
    }

    async initialize(config = {}) {
        const spinner = ora('LLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÇíÂàùÊúüÂåñ‰∏≠...').start();

        try {
            // Ë®≠ÂÆö„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø
            const configPath = config.configPath || 'llm/config/providers.json';
            const providersConfig = JSON.parse(
                await fs.readFile(configPath, 'utf-8').catch(() => '{}')
            );

            // „Éó„É≠„Éê„Ç§„ÉÄ„ÉºÂàùÊúüÂåñ
            if (providersConfig.claude?.enabled) {
                this.addProvider('claude', new ClaudeProvider(providersConfig.claude));
            }
            if (providersConfig.openai?.enabled) {
                this.addProvider('openai', new OpenAIProvider(providersConfig.openai));
            }
            if (providersConfig.gemini?.enabled) {
                this.addProvider('gemini', new GeminiProvider(providersConfig.gemini));
            }
            if (providersConfig.huggingface?.enabled) {
                this.addProvider('huggingface', new HuggingFaceProvider(providersConfig.huggingface));
            }
            if (providersConfig.ollama?.enabled) {
                this.addProvider('ollama', new OllamaProvider(providersConfig.ollama));
            }

            // „Éá„Éï„Ç©„É´„Éà„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÔºà„É¢„ÉÉ„ÇØÔºâ
            if (this.providers.size === 0) {
                this.addProvider('mock', new MockProvider());
            }

            spinner.succeed(`${this.providers.size} „Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÇíÂàùÊúüÂåñ„Åó„Åæ„Åó„Åü`);
        } catch (error) {
            spinner.fail('ÂàùÊúüÂåñ„Ç®„É©„Éº');
            throw error;
        }
    }

    addProvider(name, provider) {
        this.providers.set(name, provider);
    }

    setStrategy(strategyName) {
        if (!this.strategies[strategyName]) {
            throw new Error(`Unknown strategy: ${strategyName}`);
        }
        this.currentStrategy = strategyName;
    }

    async complete(prompt, options = {}) {
        const strategy = this.strategies[this.currentStrategy];
        const provider = await strategy.selectProvider(this.providers, options);

        if (!provider) {
            throw new Error('No available providers');
        }

        console.log(chalk.gray(`Using ${provider.name} with ${this.currentStrategy} strategy`));

        const result = await provider.complete(prompt, options);

        // Â±•Ê≠¥Ë®òÈå≤
        this.history.push({
            timestamp: new Date().toISOString(),
            provider: provider.name,
            strategy: this.currentStrategy,
            latency: result.latency,
            tokens: result.usage.totalTokens
        });

        return result;
    }

    async compareProviders(prompt, options = {}) {
        console.log(chalk.bold('\nüî¨ „Éó„É≠„Éê„Ç§„ÉÄ„ÉºÊØîËºÉ:\n'));

        const results = [];

        for (const [name, provider] of this.providers) {
            try {
                console.log(`Testing ${name}...`);
                const result = await provider.complete(prompt, options);
                results.push(result);
                
                console.log(chalk.green(`‚úì ${name}: ${result.latency}ms`));
            } catch (error) {
                console.log(chalk.red(`‚úó ${name}: ${error.message}`));
            }
        }

        return results;
    }

    getMetrics() {
        const metrics = {};

        for (const [name, provider] of this.providers) {
            metrics[name] = provider.getMetrics();
        }

        return metrics;
    }
}

// „Çπ„Éà„É©„ÉÜ„Ç∏„ÉºÂü∫Â∫ï„ÇØ„É©„Çπ
class Strategy {
    async selectProvider(providers, options) {
        throw new Error('Subclass must implement selectProvider()');
    }
}

// „Éï„Çß„Ç§„É´„Ç™„Éº„Éê„ÉºÊà¶Áï•
class FailoverStrategy extends Strategy {
    async selectProvider(providers, options) {
        const priority = options.priority || ['claude', 'openai', 'gemini', 'huggingface', 'ollama', 'mock'];

        for (const name of priority) {
            const provider = providers.get(name);
            if (provider) {
                return provider;
            }
        }

        return providers.values().next().value;
    }
}

// Ë≤†Ëç∑ÂàÜÊï£Êà¶Áï•
class LoadBalanceStrategy extends Strategy {
    constructor() {
        super();
        this.index = 0;
    }

    async selectProvider(providers) {
        const providerArray = Array.from(providers.values());
        const provider = providerArray[this.index % providerArray.length];
        this.index++;
        return provider;
    }
}

// „Ç≥„Çπ„ÉàÊúÄÈÅ©ÂåñÊà¶Áï•
class CostOptimizeStrategy extends Strategy {
    async selectProvider(providers, options) {
        // ÂìÅË≥™Ë¶ÅÊ±Ç„Å´Âøú„Åò„Å¶ÊúÄ„ÇÇÂÆâ„ÅÑ„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÇíÈÅ∏Êäû
        const quality = options.quality || 'fast';
        
        if (quality === 'fast') {
            // ÂÆâ„Åè„Å¶ÈÄü„ÅÑ„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÇíÂÑ™ÂÖà
            return providers.get('ollama') || 
                   providers.get('huggingface') || 
                   providers.get('gemini') ||
                   providers.values().next().value;
        }
        
        return providers.get('claude') || 
               providers.get('openai') || 
               providers.values().next().value;
    }
}

// ÂìÅË≥™ÂÑ™ÂÖàÊà¶Áï•
class QualityFirstStrategy extends Strategy {
    async selectProvider(providers) {
        return providers.get('claude') || 
               providers.get('openai') || 
               providers.get('gemini') ||
               providers.values().next().value;
    }
}

// ÈÄüÂ∫¶ÂÑ™ÂÖàÊà¶Áï•
class SpeedFirstStrategy extends Strategy {
    async selectProvider(providers) {
        // „É≠„Éº„Ç´„É´„É¢„Éá„É´„ÇíÂÑ™ÂÖà
        return providers.get('ollama') || 
               providers.get('huggingface') || 
               providers.values().next().value;
    }
}

// „É¢„ÉÉ„ÇØ„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÔºà„ÉÜ„Çπ„ÉàÁî®Ôºâ
class MockProvider extends LLMProvider {
    constructor() {
        super('Mock', {});
    }

    async complete(prompt, options = {}) {
        const startTime = Date.now();
        
        // Êì¨‰ººÁöÑ„Å™ÈÅÖÂª∂
        await new Promise(resolve => setTimeout(resolve, 100));
        
        const latency = Date.now() - startTime;
        
        this.recordMetrics(100, latency);

        return {
            content: `Mock response for: ${prompt.substring(0, 50)}...`,
            model: 'mock-model',
            provider: this.name,
            usage: {
                promptTokens: 50,
                completionTokens: 50,
                totalTokens: 100
            },
            latency
        };
    }
}

// CLI
async function main() {
    const manager = new LLMManager();
    await manager.initialize();

    const command = process.argv[2];
    const prompt = process.argv.slice(3).join(' ');

    switch (command) {
        case 'complete':
            const result = await manager.complete(prompt);
            console.log('\n' + chalk.green('Response:'));
            console.log(result.content);
            console.log(chalk.gray(`\n[${result.provider}/${result.model}] ${result.latency}ms`));
            break;

        case 'compare':
            await manager.compareProviders(prompt);
            break;

        case 'metrics':
            const metrics = manager.getMetrics();
            console.log('\n' + chalk.bold('Provider Metrics:'));
            console.log(JSON.stringify(metrics, null, 2));
            break;

        default:
            console.log(`
‰ΩøÁî®ÊñπÊ≥ï:
  node llm-manager.js complete <prompt>  - LLMË£úÂÆå
  node llm-manager.js compare <prompt>   - „Éó„É≠„Éê„Ç§„ÉÄ„ÉºÊØîËºÉ
  node llm-manager.js metrics            - „É°„Éà„É™„ÇØ„ÇπË°®Á§∫
            `);
    }
}

if (process.argv[1] === new URL(import.meta.url).pathname) {
    main().catch(console.error);
}

export default LLMManager;
LLMMANAGER_EOF

# 3. „Éó„É≠„Éê„Ç§„ÉÄ„ÉºË®≠ÂÆö„Éï„Ç°„Ç§„É´
cat > llm/config/providers.json << 'LLMCONFIG_EOF'
{
  "claude": {
    "enabled": false,
    "apiKey": "${ANTHROPIC_API_KEY}",
    "models": {
      "fast": "claude-3-haiku-20240307",
      "balanced": "claude-3-sonnet-20240229",
      "powerful": "claude-3-opus-20240229"
    },
    "costPerMillion": {
      "input": 0.25,
      "output": 1.25
    }
  },
  "openai": {
    "enabled": false,
    "apiKey": "${OPENAI_API_KEY}",
    "models": {
      "fast": "gpt-3.5-turbo",
      "balanced": "gpt-4-turbo-preview",
      "powerful": "gpt-4"
    },
    "costPerMillion": {
      "input": 0.50,
      "output": 1.50
    }
  },
  "gemini": {
    "enabled": false,
    "apiKey": "${GOOGLE_API_KEY}",
    "models": {
      "fast": "gemini-1.5-flash",
      "balanced": "gemini-1.5-pro",
      "powerful": "gemini-1.5-pro"
    },
    "costPerMillion": {
      "input": 0.00,
      "output": 0.00
    }
  },
  "huggingface": {
    "enabled": false,
    "apiKey": "${HUGGINGFACE_API_KEY}",
    "models": {
      "fast": "microsoft/phi-2",
      "balanced": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "powerful": "meta-llama/Llama-2-70b-chat-hf"
    }
  },
  "ollama": {
    "enabled": true,
    "baseUrl": "http://localhost:11434",
    "models": {
      "fast": "phi",
      "balanced": "mistral",
      "powerful": "llama2:70b"
    }
  }
}
LLMCONFIG_EOF

# 4. Áµ±Âêà„Çπ„ÇØ„É™„Éó„Éà
cat > llm/setup-providers.sh << 'SETUP_EOF'
#!/usr/bin/env bash
# LLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
set -euo pipefail

echo "ü§ñ LLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„Çí„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó‰∏≠..."

# Áí∞Â¢ÉÂ§âÊï∞„ÉÅ„Çß„ÉÉ„ÇØ
check_env() {
    local var=$1
    local name=$2
    
    if [[ -n "${!var:-}" ]]; then
        echo "‚úÖ $name: Ë®≠ÂÆöÊ∏à„Åø"
        return 0
    else
        echo "‚ö†Ô∏è  $name: Êú™Ë®≠ÂÆö (export $var=your-key)"
        return 1
    fi
}

echo ""
echo "Áí∞Â¢ÉÂ§âÊï∞„ÉÅ„Çß„ÉÉ„ÇØ:"
check_env "ANTHROPIC_API_KEY" "Claude" || true
check_env "OPENAI_API_KEY" "OpenAI" || true
check_env "GOOGLE_API_KEY" "Gemini" || true
check_env "HUGGINGFACE_API_KEY" "HuggingFace" || true

# Ollama„ÉÅ„Çß„ÉÉ„ÇØ
echo ""
echo "„É≠„Éº„Ç´„É´LLM„ÉÅ„Çß„ÉÉ„ÇØ:"
if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
    echo "‚úÖ Ollama: Ëµ∑Âãï‰∏≠"
    echo "  Âà©Áî®ÂèØËÉΩ„Å™„É¢„Éá„É´:"
    curl -s http://localhost:11434/api/tags | python3 -m json.tool | grep '"name"' | head -5
else
    echo "‚ö†Ô∏è  Ollama: Êú™Ëµ∑Âãï"
    echo "  „Ç§„É≥„Çπ„Éà„Éº„É´: https://ollama.ai"
    echo "  Ëµ∑Âãï: ollama serve"
    echo "  „É¢„Éá„É´ÂèñÂæó: ollama pull mistral"
fi

# Ë®≠ÂÆö„Éï„Ç°„Ç§„É´Êõ¥Êñ∞
echo ""
read -p "Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÇíÁí∞Â¢ÉÂ§âÊï∞„ÅßÊõ¥Êñ∞„Åó„Åæ„Åô„Åã? (y/N): " update
if [[ "$update" == "y" ]]; then
    # Áí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö„Éï„Ç°„Ç§„É´„Å´ÂèçÊò†
    envsubst < config/providers.json > config/providers.json.tmp
    mv config/providers.json.tmp config/providers.json
    echo "‚úÖ Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÇíÊõ¥Êñ∞„Åó„Åæ„Åó„Åü"
fi

echo ""
echo "„Çª„ÉÉ„Éà„Ç¢„ÉÉ„ÉóÂÆå‰∫ÜÔºÅ"
echo "„ÉÜ„Çπ„Éà: node llm-manager.js complete 'Hello, world!'"
SETUP_EOF

# 5. package.json
cat > llm/package.json << 'LLM_PACKAGE_EOF'
{
  "name": "subagent-multi-llm",
  "version": "1.0.0",
  "description": "Multi-LLM provider support for SubAgent",
  "type": "module",
  "scripts": {
    "setup": "bash setup-providers.sh",
    "test": "node llm-manager.js complete 'Test prompt'",
    "compare": "node llm-manager.js compare",
    "metrics": "node llm-manager.js metrics"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.20.0",
    "openai": "^4.28.0",
    "@google/generative-ai": "^0.7.1",
    "@huggingface/inference": "^2.6.4",
    "replicate": "^0.29.1",
    "chalk": "^5.3.0",
    "ora": "^7.0.1"
  }
}
LLM_PACKAGE_EOF

chmod +x llm/*.sh

echo "‚úÖ Ë§áÊï∞LLM„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÂØæÂøú„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ"
echo ""
echo "ü§ñ ÂØæÂøú„Éó„É≠„Éê„Ç§„ÉÄ„Éº:"
echo "  - Claude (Anthropic)"
echo "  - GPT (OpenAI)"
echo "  - Gemini (Google)"
echo "  - HuggingFace"
echo "  - Ollama („É≠„Éº„Ç´„É´)"
echo ""
echo "üéØ Êà¶Áï•:"
echo "  - failover: „Éï„Çß„Ç§„É´„Ç™„Éº„Éê„Éº"
echo "  - loadBalance: Ë≤†Ëç∑ÂàÜÊï£"
echo "  - costOptimize: „Ç≥„Çπ„ÉàÊúÄÈÅ©Âåñ"
echo "  - quality: ÂìÅË≥™ÂÑ™ÂÖà"
echo "  - speed: ÈÄüÂ∫¶ÂÑ™ÂÖà"
echo ""
echo "üöÄ ‰Ωø„ÅÑÊñπ:"
echo "  cd llm && npm install"
echo "  npm run setup                    # „Éó„É≠„Éê„Ç§„ÉÄ„ÉºË®≠ÂÆö"
echo "  npm test                         # „ÉÜ„Çπ„ÉàÂÆüË°å"
echo "  npm run compare -- '„Éó„É≠„É≥„Éó„Éà'  # „Éó„É≠„Éê„Ç§„ÉÄ„ÉºÊØîËºÉ"